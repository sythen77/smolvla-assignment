{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f2b64b2d0cb24d0794971f0d8c378796": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_6aebce1e34d44d0f8a8d3f5eb67ecf54"
          }
        },
        "c8746e47049649d1b68541e5993797e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_acd7ced00aad491284bd613865241dde",
            "placeholder": "​",
            "style": "IPY_MODEL_9c4bc834b3514c90afdb5429acd82f1c",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "b740dccb4882429794bea90c53abb3b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_5e9a48af5fb04c6e8d1a1fced965043a",
            "placeholder": "​",
            "style": "IPY_MODEL_8068d7c47bfd4d41bfb49f8a83be29ef",
            "value": ""
          }
        },
        "97748dc3812747ec9a0c47569a546370": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_50cdeb2d5a7d48bda8b46909988488e1",
            "style": "IPY_MODEL_dbd376ff604048e1a95da64f2ae9c58b",
            "value": true
          }
        },
        "3df43a774e904492a0eb08e453d7dcee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_c1b72976bbdc4685a70226450bbfe471",
            "style": "IPY_MODEL_d2b1d3028f014d10a7f02492ebdeabc5",
            "tooltip": ""
          }
        },
        "59a8c03ede2646eca6bfdc4de9000ecb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ffb3e8cad95472f9095bce559dc1ebd",
            "placeholder": "​",
            "style": "IPY_MODEL_785aa65d8ad140dc82a44e4f29e64bbf",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "6aebce1e34d44d0f8a8d3f5eb67ecf54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "acd7ced00aad491284bd613865241dde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c4bc834b3514c90afdb5429acd82f1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e9a48af5fb04c6e8d1a1fced965043a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8068d7c47bfd4d41bfb49f8a83be29ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "50cdeb2d5a7d48bda8b46909988488e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbd376ff604048e1a95da64f2ae9c58b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1b72976bbdc4685a70226450bbfe471": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2b1d3028f014d10a7f02492ebdeabc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "5ffb3e8cad95472f9095bce559dc1ebd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "785aa65d8ad140dc82a44e4f29e64bbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57b37865687a4327957c6dbebb40943d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41cd725bb6e34d85ba14d3494e45616f",
            "placeholder": "​",
            "style": "IPY_MODEL_8382aafb88cb4c40b3467cc438280275",
            "value": "Connecting..."
          }
        },
        "41cd725bb6e34d85ba14d3494e45616f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8382aafb88cb4c40b3467cc438280275": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "effb187eac7d4e83af979e654dc57708": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_c92cf06721954169bc4d85e2c4bf3c1c"
          }
        },
        "cfc4541ed6d94d30999fe669215d678a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a8f812ef68d4ea581171c6fb7cd2606",
            "placeholder": "​",
            "style": "IPY_MODEL_82ac737a80df447399137592bfc03d9f",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "632358c60e4e411ebb54b2bedd870ae2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_fb32432db7bb46f8b27cb34376a11366",
            "placeholder": "​",
            "style": "IPY_MODEL_a30b0963e33f4be4b597f3e77675fe4e",
            "value": ""
          }
        },
        "74269301ef21404bbc53e3543594ae3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_5b354b7a3ea542c5968db1dd11512fb2",
            "style": "IPY_MODEL_3c1446f4fb4440c09187c48f4ce6509a",
            "value": true
          }
        },
        "6b6affaaf53940d2928e2431e050780a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_78291b45468c42af8732de5effccb6f8",
            "style": "IPY_MODEL_5701b36a5b9b493db54bc190c2604011",
            "tooltip": ""
          }
        },
        "972fe68c83914d32b36cfdd2ea642a3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b08d5c1aae24f41930486ddc1fd5f1e",
            "placeholder": "​",
            "style": "IPY_MODEL_bb112a03732b4dbcbdfea7a575335d8f",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "c92cf06721954169bc4d85e2c4bf3c1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "7a8f812ef68d4ea581171c6fb7cd2606": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82ac737a80df447399137592bfc03d9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb32432db7bb46f8b27cb34376a11366": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a30b0963e33f4be4b597f3e77675fe4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b354b7a3ea542c5968db1dd11512fb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c1446f4fb4440c09187c48f4ce6509a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78291b45468c42af8732de5effccb6f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5701b36a5b9b493db54bc190c2604011": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "7b08d5c1aae24f41930486ddc1fd5f1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb112a03732b4dbcbdfea7a575335d8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "23b3c9f2d1564028b1752e0868c27510": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd0fed37ee0b4910864ec1b6336a7af8",
            "placeholder": "​",
            "style": "IPY_MODEL_0a17389f281946d68076fd63698a552d",
            "value": "Connecting..."
          }
        },
        "bd0fed37ee0b4910864ec1b6336a7af8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a17389f281946d68076fd63698a552d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyloNZuXOx3S",
        "outputId": "17a09761-82c8-40cc-a7d4-1ae927c8b065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!ls\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MI7cohK_O2CC",
        "outputId": "5c5763d3-ea81-495c-cd29-c0df931d63af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/huggingface/lerobot.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfox4MSyPGZq",
        "outputId": "8dd105eb-926a-4c1a-a9f9-f86a1a3652bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lerobot'...\n",
            "remote: Enumerating objects: 45396, done.\u001b[K\n",
            "remote: Counting objects: 100% (407/407), done.\u001b[K\n",
            "remote: Compressing objects: 100% (188/188), done.\u001b[K\n",
            "remote: Total 45396 (delta 333), reused 219 (delta 219), pack-reused 44989 (from 4)\u001b[K\n",
            "Receiving objects: 100% (45396/45396), 232.03 MiB | 17.42 MiB/s, done.\n",
            "Resolving deltas: 100% (29234/29234), done.\n",
            "Filtering content: 100% (45/45), 69.03 MiB | 16.53 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/lerobot\n",
        "!ls\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-87CliJPTgQ",
        "outputId": "9ad4a95d-7002-413d-c22a-7c53050d447a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/lerobot\n",
            "benchmarks\t       examples        README.md\n",
            "CODE_OF_CONDUCT.md     LICENSE\t       requirements.in\n",
            "CONTRIBUTING.md        Makefile        requirements-macos.txt\n",
            "docker\t\t       MANIFEST.in     requirements-ubuntu.txt\n",
            "docs\t\t       media\t       src\n",
            "docs-requirements.txt  pyproject.toml  tests\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U \"lerobot[smolvla]\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2ctNyWvPjBp",
        "outputId": "bd3096c6-f4e3-4efa-9b49-cdb29bf9388f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.0/821.0 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m758.8/758.8 kB\u001b[0m \u001b[31m873.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for evdev (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"import shutil; print('lerobot-train =', shutil.which('lerobot-train'))\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quarqAzyQs3f",
        "outputId": "12684f7f-7a5b-4d54-fd5c-2992d5613fb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lerobot-train = /usr/local/bin/lerobot-train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/outputs/train\n",
        "\n",
        "!lerobot-train \\\n",
        "  --policy.path=lerobot/smolvla_base \\\n",
        "  --policy.repo_id=Sythen/my_smolvla_training_fm \\\n",
        "  --dataset.repo_id=Sythen/svla_so100_pickplace_copy \\\n",
        "  --batch_size=8 \\\n",
        "  --steps=200 \\\n",
        "  --output_dir=/content/outputs/train/my_smolvla \\\n",
        "  --job_name=my_smolvla_training_test \\\n",
        "  --policy.device=cuda \\\n",
        "  --wandb.enable=false\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGSKlOnrQ25S",
        "outputId": "c70d921d-3f80-4eb8-aacb-559596d87132"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-22 04:14:09.274633: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766376849.568485    3770 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766376849.656552    3770 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766376850.245723    3770 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766376850.245769    3770 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766376850.245774    3770 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766376850.245778    3770 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-22 04:14:10.301959: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "config.json: 2.30kB [00:00, 12.2MB/s]\n",
            "INFO 2025-12-22 04:14:23 ot_train.py:163 {'batch_size': 8,\n",
            " 'checkpoint_path': None,\n",
            " 'dataset': {'episodes': None,\n",
            "             'image_transforms': {'enable': False,\n",
            "                                  'max_num_transforms': 3,\n",
            "                                  'random_order': False,\n",
            "                                  'tfs': {'affine': {'kwargs': {'degrees': [-5.0,\n",
            "                                                                            5.0],\n",
            "                                                                'translate': [0.05,\n",
            "                                                                              0.05]},\n",
            "                                                     'type': 'RandomAffine',\n",
            "                                                     'weight': 1.0},\n",
            "                                          'brightness': {'kwargs': {'brightness': [0.8,\n",
            "                                                                                   1.2]},\n",
            "                                                         'type': 'ColorJitter',\n",
            "                                                         'weight': 1.0},\n",
            "                                          'contrast': {'kwargs': {'contrast': [0.8,\n",
            "                                                                               1.2]},\n",
            "                                                       'type': 'ColorJitter',\n",
            "                                                       'weight': 1.0},\n",
            "                                          'hue': {'kwargs': {'hue': [-0.05,\n",
            "                                                                     0.05]},\n",
            "                                                  'type': 'ColorJitter',\n",
            "                                                  'weight': 1.0},\n",
            "                                          'saturation': {'kwargs': {'saturation': [0.5,\n",
            "                                                                                   1.5]},\n",
            "                                                         'type': 'ColorJitter',\n",
            "                                                         'weight': 1.0},\n",
            "                                          'sharpness': {'kwargs': {'sharpness': [0.5,\n",
            "                                                                                 1.5]},\n",
            "                                                        'type': 'SharpnessJitter',\n",
            "                                                        'weight': 1.0}}},\n",
            "             'repo_id': 'Sythen/svla_so100_pickplace_copy',\n",
            "             'revision': None,\n",
            "             'root': None,\n",
            "             'streaming': False,\n",
            "             'use_imagenet_stats': True,\n",
            "             'video_backend': 'torchcodec'},\n",
            " 'env': None,\n",
            " 'eval': {'batch_size': 50, 'n_episodes': 50, 'use_async_envs': False},\n",
            " 'eval_freq': 20000,\n",
            " 'job_name': 'my_smolvla_training_test',\n",
            " 'log_freq': 200,\n",
            " 'num_workers': 4,\n",
            " 'optimizer': {'betas': [0.9, 0.95],\n",
            "               'eps': 1e-08,\n",
            "               'grad_clip_norm': 10.0,\n",
            "               'lr': 0.0001,\n",
            "               'type': 'adamw',\n",
            "               'weight_decay': 1e-10},\n",
            " 'output_dir': '/content/outputs/train/my_smolvla',\n",
            " 'policy': {'adapt_to_pi_aloha': False,\n",
            "            'add_image_special_tokens': False,\n",
            "            'attention_mode': 'cross_attn',\n",
            "            'chunk_size': 50,\n",
            "            'device': 'cuda',\n",
            "            'empty_cameras': 0,\n",
            "            'expert_width_multiplier': 0.75,\n",
            "            'freeze_vision_encoder': True,\n",
            "            'input_features': {'observation.images.camera1': {'shape': [3,\n",
            "                                                                        256,\n",
            "                                                                        256],\n",
            "                                                              'type': <FeatureType.VISUAL: 'VISUAL'>},\n",
            "                               'observation.images.camera2': {'shape': [3,\n",
            "                                                                        256,\n",
            "                                                                        256],\n",
            "                                                              'type': <FeatureType.VISUAL: 'VISUAL'>},\n",
            "                               'observation.images.camera3': {'shape': [3,\n",
            "                                                                        256,\n",
            "                                                                        256],\n",
            "                                                              'type': <FeatureType.VISUAL: 'VISUAL'>},\n",
            "                               'observation.state': {'shape': [6],\n",
            "                                                     'type': <FeatureType.STATE: 'STATE'>}},\n",
            "            'license': None,\n",
            "            'load_vlm_weights': True,\n",
            "            'max_action_dim': 32,\n",
            "            'max_period': 4.0,\n",
            "            'max_state_dim': 32,\n",
            "            'min_period': 0.004,\n",
            "            'n_action_steps': 50,\n",
            "            'n_obs_steps': 1,\n",
            "            'normalization_mapping': {'ACTION': <NormalizationMode.MEAN_STD: 'MEAN_STD'>,\n",
            "                                      'STATE': <NormalizationMode.MEAN_STD: 'MEAN_STD'>,\n",
            "                                      'VISUAL': <NormalizationMode.IDENTITY: 'IDENTITY'>},\n",
            "            'num_expert_layers': 0,\n",
            "            'num_steps': 10,\n",
            "            'num_vlm_layers': 16,\n",
            "            'optimizer_betas': [0.9, 0.95],\n",
            "            'optimizer_eps': 1e-08,\n",
            "            'optimizer_grad_clip_norm': 10.0,\n",
            "            'optimizer_lr': 0.0001,\n",
            "            'optimizer_weight_decay': 1e-10,\n",
            "            'output_features': {'action': {'shape': [6],\n",
            "                                           'type': <FeatureType.ACTION: 'ACTION'>}},\n",
            "            'pad_language_to': 'max_length',\n",
            "            'prefix_length': 0,\n",
            "            'pretrained_path': 'lerobot/smolvla_base',\n",
            "            'private': None,\n",
            "            'push_to_hub': True,\n",
            "            'repo_id': 'Sythen/my_smolvla_training_fm',\n",
            "            'resize_imgs_with_padding': [512, 512],\n",
            "            'rtc_config': None,\n",
            "            'scheduler_decay_lr': 2.5e-06,\n",
            "            'scheduler_decay_steps': 30000,\n",
            "            'scheduler_warmup_steps': 1000,\n",
            "            'self_attn_every_n_layers': 2,\n",
            "            'tags': None,\n",
            "            'tokenizer_max_length': 48,\n",
            "            'train_expert_only': True,\n",
            "            'train_state_proj': True,\n",
            "            'type': 'smolvla',\n",
            "            'use_amp': False,\n",
            "            'use_cache': True,\n",
            "            'use_delta_joint_actions_aloha': False,\n",
            "            'vlm_model_name': 'HuggingFaceTB/SmolVLM2-500M-Video-Instruct'},\n",
            " 'rename_map': {},\n",
            " 'resume': False,\n",
            " 'save_checkpoint': True,\n",
            " 'save_freq': 20000,\n",
            " 'scheduler': {'decay_lr': 2.5e-06,\n",
            "               'num_decay_steps': 30000,\n",
            "               'num_warmup_steps': 1000,\n",
            "               'peak_lr': 0.0001,\n",
            "               'type': 'cosine_decay_with_warmup'},\n",
            " 'seed': 1000,\n",
            " 'steps': 200,\n",
            " 'use_policy_training_preset': True,\n",
            " 'wandb': {'disable_artifact': False,\n",
            "           'enable': False,\n",
            "           'entity': None,\n",
            "           'mode': None,\n",
            "           'notes': None,\n",
            "           'project': 'lerobot',\n",
            "           'run_id': None}}\n",
            "INFO 2025-12-22 04:14:23 ot_train.py:171 \u001b[1m\u001b[33mLogs will be saved locally.\u001b[0m\n",
            "INFO 2025-12-22 04:14:23 ot_train.py:183 Creating dataset\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lerobot/datasets/lerobot_dataset.py\", line 103, in __init__\n",
            "    self.load_metadata()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lerobot/datasets/lerobot_dataset.py\", line 161, in load_metadata\n",
            "    self.info = load_info(self.root)\n",
            "                ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lerobot/datasets/utils.py\", line 298, in load_info\n",
            "    info = load_json(local_dir / INFO_PATH)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lerobot/datasets/utils.py\", line 265, in load_json\n",
            "    with open(fpath) as f:\n",
            "         ^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/root/.cache/huggingface/lerobot/Sythen/svla_so100_pickplace_copy/meta/info.json'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/lerobot-train\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lerobot/scripts/lerobot_train.py\", line 449, in main\n",
            "    train()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lerobot/configs/parser.py\", line 233, in wrapper_inner\n",
            "    response = fn(cfg, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lerobot/scripts/lerobot_train.py\", line 184, in train\n",
            "    dataset = make_dataset(cfg)\n",
            "              ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lerobot/datasets/factory.py\", line 88, in make_dataset\n",
            "    ds_meta = LeRobotDatasetMetadata(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lerobot/datasets/lerobot_dataset.py\", line 106, in __init__\n",
            "    self.revision = get_safe_version(self.repo_id, self.revision)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lerobot/datasets/utils.py\", line 532, in get_safe_version\n",
            "    raise RevisionNotFoundError(\n",
            "huggingface_hub.errors.RevisionNotFoundError: Your dataset must be tagged with a codebase version.\n",
            "            Assuming _version_ is the codebase_version value in the info.json, you can run this:\n",
            "            ```python\n",
            "            from huggingface_hub import HfApi\n",
            "\n",
            "            hub_api = HfApi()\n",
            "            hub_api.create_tag(\"Sythen/svla_so100_pickplace_copy\", tag=\"_version_\", repo_type=\"dataset\")\n",
            "            ```\n",
            "            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "f2b64b2d0cb24d0794971f0d8c378796",
            "c8746e47049649d1b68541e5993797e8",
            "b740dccb4882429794bea90c53abb3b6",
            "97748dc3812747ec9a0c47569a546370",
            "3df43a774e904492a0eb08e453d7dcee",
            "59a8c03ede2646eca6bfdc4de9000ecb",
            "6aebce1e34d44d0f8a8d3f5eb67ecf54",
            "acd7ced00aad491284bd613865241dde",
            "9c4bc834b3514c90afdb5429acd82f1c",
            "5e9a48af5fb04c6e8d1a1fced965043a",
            "8068d7c47bfd4d41bfb49f8a83be29ef",
            "50cdeb2d5a7d48bda8b46909988488e1",
            "dbd376ff604048e1a95da64f2ae9c58b",
            "c1b72976bbdc4685a70226450bbfe471",
            "d2b1d3028f014d10a7f02492ebdeabc5",
            "5ffb3e8cad95472f9095bce559dc1ebd",
            "785aa65d8ad140dc82a44e4f29e64bbf",
            "57b37865687a4327957c6dbebb40943d",
            "41cd725bb6e34d85ba14d3494e45616f",
            "8382aafb88cb4c40b3467cc438280275"
          ]
        },
        "id": "aQMJ6nN_RH2o",
        "outputId": "086b27c0-ec93-41ff-be3e-82c9e751a442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f2b64b2d0cb24d0794971f0d8c378796"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "hub_api = HfApi()\n",
        "hub_api.create_tag(\n",
        "    \"Sythen/svla_so100_pickplace_copy\",\n",
        "    tag=\"_version_\",\n",
        "    repo_type=\"dataset\"\n",
        ")\n",
        "print(\"✅ Created _version_ tag\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTFTk4wwRiO_",
        "outputId": "fe4b0c49-7ce4-4fb8-b747-14cf2509df54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Created _version_ tag\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "effb187eac7d4e83af979e654dc57708",
            "cfc4541ed6d94d30999fe669215d678a",
            "632358c60e4e411ebb54b2bedd870ae2",
            "74269301ef21404bbc53e3543594ae3a",
            "6b6affaaf53940d2928e2431e050780a",
            "972fe68c83914d32b36cfdd2ea642a3b",
            "c92cf06721954169bc4d85e2c4bf3c1c",
            "7a8f812ef68d4ea581171c6fb7cd2606",
            "82ac737a80df447399137592bfc03d9f",
            "fb32432db7bb46f8b27cb34376a11366",
            "a30b0963e33f4be4b597f3e77675fe4e",
            "5b354b7a3ea542c5968db1dd11512fb2",
            "3c1446f4fb4440c09187c48f4ce6509a",
            "78291b45468c42af8732de5effccb6f8",
            "5701b36a5b9b493db54bc190c2604011",
            "7b08d5c1aae24f41930486ddc1fd5f1e",
            "bb112a03732b4dbcbdfea7a575335d8f",
            "23b3c9f2d1564028b1752e0868c27510",
            "bd0fed37ee0b4910864ec1b6336a7af8",
            "0a17389f281946d68076fd63698a552d"
          ]
        },
        "id": "JPTQszG4Rb1b",
        "outputId": "bf6e43e6-d089-4033-ddc9-0af43cd37654"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "effb187eac7d4e83af979e654dc57708"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi()\n",
        "api.create_tag(\n",
        "    repo_id=\"Sythen/svla_so100_pickplace_copy\",\n",
        "    tag=\"_version_\",\n",
        "    repo_type=\"dataset\",\n",
        "    revision=\"main\",\n",
        "    exist_ok=True\n",
        ")\n",
        "print(\"✅ _version_ tag updated to main\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpJv88oySDOJ",
        "outputId": "43a9876a-d97a-46ab-b90e-a8e996672b4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ _version_ tag updated to main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/outputs/train\n",
        "\n",
        "!lerobot-train \\\n",
        "  --policy.path=lerobot/smolvla_base \\\n",
        "  --policy.repo_id=Sythen/my_smolvla_training_fm \\\n",
        "  --dataset.repo_id=lerobot/svla_so100_pickplace \\\n",
        "  --batch_size=8 \\\n",
        "  --steps=200 \\\n",
        "  --output_dir=/content/outputs/train/my_smolvla \\\n",
        "  --job_name=my_smolvla_training_test \\\n",
        "  --policy.device=cuda \\\n",
        "  --wandb.enable=false\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_jxLvPbSYau",
        "outputId": "fe756f08-cc93-4756-f5dc-76aad4b69f35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-22 04:23:13.489499: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766377393.510515    6167 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766377393.516779    6167 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766377393.532984    6167 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766377393.533012    6167 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766377393.533015    6167 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766377393.533019    6167 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-22 04:23:13.537687: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 2025-12-22 04:23:24 ot_train.py:163 {'batch_size': 8,\n",
            " 'checkpoint_path': None,\n",
            " 'dataset': {'episodes': None,\n",
            "             'image_transforms': {'enable': False,\n",
            "                                  'max_num_transforms': 3,\n",
            "                                  'random_order': False,\n",
            "                                  'tfs': {'affine': {'kwargs': {'degrees': [-5.0,\n",
            "                                                                            5.0],\n",
            "                                                                'translate': [0.05,\n",
            "                                                                              0.05]},\n",
            "                                                     'type': 'RandomAffine',\n",
            "                                                     'weight': 1.0},\n",
            "                                          'brightness': {'kwargs': {'brightness': [0.8,\n",
            "                                                                                   1.2]},\n",
            "                                                         'type': 'ColorJitter',\n",
            "                                                         'weight': 1.0},\n",
            "                                          'contrast': {'kwargs': {'contrast': [0.8,\n",
            "                                                                               1.2]},\n",
            "                                                       'type': 'ColorJitter',\n",
            "                                                       'weight': 1.0},\n",
            "                                          'hue': {'kwargs': {'hue': [-0.05,\n",
            "                                                                     0.05]},\n",
            "                                                  'type': 'ColorJitter',\n",
            "                                                  'weight': 1.0},\n",
            "                                          'saturation': {'kwargs': {'saturation': [0.5,\n",
            "                                                                                   1.5]},\n",
            "                                                         'type': 'ColorJitter',\n",
            "                                                         'weight': 1.0},\n",
            "                                          'sharpness': {'kwargs': {'sharpness': [0.5,\n",
            "                                                                                 1.5]},\n",
            "                                                        'type': 'SharpnessJitter',\n",
            "                                                        'weight': 1.0}}},\n",
            "             'repo_id': 'lerobot/svla_so100_pickplace',\n",
            "             'revision': None,\n",
            "             'root': None,\n",
            "             'streaming': False,\n",
            "             'use_imagenet_stats': True,\n",
            "             'video_backend': 'torchcodec'},\n",
            " 'env': None,\n",
            " 'eval': {'batch_size': 50, 'n_episodes': 50, 'use_async_envs': False},\n",
            " 'eval_freq': 20000,\n",
            " 'job_name': 'my_smolvla_training_test',\n",
            " 'log_freq': 200,\n",
            " 'num_workers': 4,\n",
            " 'optimizer': {'betas': [0.9, 0.95],\n",
            "               'eps': 1e-08,\n",
            "               'grad_clip_norm': 10.0,\n",
            "               'lr': 0.0001,\n",
            "               'type': 'adamw',\n",
            "               'weight_decay': 1e-10},\n",
            " 'output_dir': '/content/outputs/train/my_smolvla',\n",
            " 'policy': {'adapt_to_pi_aloha': False,\n",
            "            'add_image_special_tokens': False,\n",
            "            'attention_mode': 'cross_attn',\n",
            "            'chunk_size': 50,\n",
            "            'device': 'cuda',\n",
            "            'empty_cameras': 0,\n",
            "            'expert_width_multiplier': 0.75,\n",
            "            'freeze_vision_encoder': True,\n",
            "            'input_features': {'observation.images.camera1': {'shape': [3,\n",
            "                                                                        256,\n",
            "                                                                        256],\n",
            "                                                              'type': <FeatureType.VISUAL: 'VISUAL'>},\n",
            "                               'observation.images.camera2': {'shape': [3,\n",
            "                                                                        256,\n",
            "                                                                        256],\n",
            "                                                              'type': <FeatureType.VISUAL: 'VISUAL'>},\n",
            "                               'observation.images.camera3': {'shape': [3,\n",
            "                                                                        256,\n",
            "                                                                        256],\n",
            "                                                              'type': <FeatureType.VISUAL: 'VISUAL'>},\n",
            "                               'observation.state': {'shape': [6],\n",
            "                                                     'type': <FeatureType.STATE: 'STATE'>}},\n",
            "            'license': None,\n",
            "            'load_vlm_weights': True,\n",
            "            'max_action_dim': 32,\n",
            "            'max_period': 4.0,\n",
            "            'max_state_dim': 32,\n",
            "            'min_period': 0.004,\n",
            "            'n_action_steps': 50,\n",
            "            'n_obs_steps': 1,\n",
            "            'normalization_mapping': {'ACTION': <NormalizationMode.MEAN_STD: 'MEAN_STD'>,\n",
            "                                      'STATE': <NormalizationMode.MEAN_STD: 'MEAN_STD'>,\n",
            "                                      'VISUAL': <NormalizationMode.IDENTITY: 'IDENTITY'>},\n",
            "            'num_expert_layers': 0,\n",
            "            'num_steps': 10,\n",
            "            'num_vlm_layers': 16,\n",
            "            'optimizer_betas': [0.9, 0.95],\n",
            "            'optimizer_eps': 1e-08,\n",
            "            'optimizer_grad_clip_norm': 10.0,\n",
            "            'optimizer_lr': 0.0001,\n",
            "            'optimizer_weight_decay': 1e-10,\n",
            "            'output_features': {'action': {'shape': [6],\n",
            "                                           'type': <FeatureType.ACTION: 'ACTION'>}},\n",
            "            'pad_language_to': 'max_length',\n",
            "            'prefix_length': 0,\n",
            "            'pretrained_path': 'lerobot/smolvla_base',\n",
            "            'private': None,\n",
            "            'push_to_hub': True,\n",
            "            'repo_id': 'Sythen/my_smolvla_training_fm',\n",
            "            'resize_imgs_with_padding': [512, 512],\n",
            "            'rtc_config': None,\n",
            "            'scheduler_decay_lr': 2.5e-06,\n",
            "            'scheduler_decay_steps': 30000,\n",
            "            'scheduler_warmup_steps': 1000,\n",
            "            'self_attn_every_n_layers': 2,\n",
            "            'tags': None,\n",
            "            'tokenizer_max_length': 48,\n",
            "            'train_expert_only': True,\n",
            "            'train_state_proj': True,\n",
            "            'type': 'smolvla',\n",
            "            'use_amp': False,\n",
            "            'use_cache': True,\n",
            "            'use_delta_joint_actions_aloha': False,\n",
            "            'vlm_model_name': 'HuggingFaceTB/SmolVLM2-500M-Video-Instruct'},\n",
            " 'rename_map': {},\n",
            " 'resume': False,\n",
            " 'save_checkpoint': True,\n",
            " 'save_freq': 20000,\n",
            " 'scheduler': {'decay_lr': 2.5e-06,\n",
            "               'num_decay_steps': 30000,\n",
            "               'num_warmup_steps': 1000,\n",
            "               'peak_lr': 0.0001,\n",
            "               'type': 'cosine_decay_with_warmup'},\n",
            " 'seed': 1000,\n",
            " 'steps': 200,\n",
            " 'use_policy_training_preset': True,\n",
            " 'wandb': {'disable_artifact': False,\n",
            "           'enable': False,\n",
            "           'entity': None,\n",
            "           'mode': None,\n",
            "           'notes': None,\n",
            "           'project': 'lerobot',\n",
            "           'run_id': None}}\n",
            "INFO 2025-12-22 04:23:24 ot_train.py:171 \u001b[1m\u001b[33mLogs will be saved locally.\u001b[0m\n",
            "INFO 2025-12-22 04:23:24 ot_train.py:183 Creating dataset\n",
            "Fetching 4 files:   0% 0/4 [00:00<?, ?it/s]\n",
            "meta/episodes/chunk-000/file-000.parquet:   0% 0.00/71.7k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "meta/tasks.parquet:   0% 0.00/2.25k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "stats.json: 5.83kB [00:00, 8.61MB/s]\n",
            "\n",
            "\n",
            "\n",
            "info.json: 2.62kB [00:00, 9.60MB/s]\n",
            "\n",
            "\n",
            "meta/tasks.parquet: 100% 2.25k/2.25k [00:03<00:00, 657B/s]\n",
            "\n",
            "meta/episodes/chunk-000/file-000.parquet: 100% 71.7k/71.7k [00:03<00:00, 19.6kB/s]\n",
            "Fetching 4 files: 100% 4/4 [00:04<00:00,  1.05s/it]\n",
            "Fetching 9 files:   0% 0/9 [00:00<?, ?it/s]\n",
            "videos/observation.images.top/chunk-000/(…):   0% 0.00/320M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "videos/observation.images.wrist/chunk-00(…):   0% 0.00/149M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "data/chunk-000/file-000.parquet:   0% 0.00/480k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            ".gitattributes: 2.46kB [00:00, 8.98MB/s]\n",
            "Fetching 9 files:  11% 1/9 [00:00<00:05,  1.35it/s]\n",
            "\n",
            "\n",
            "\n",
            "README.md: 3.73kB [00:00, 19.5MB/s]\n",
            "\n",
            "\n",
            "\n",
            "data/chunk-000/file-000.parquet: 100% 480k/480k [00:02<00:00, 200kB/s]\n",
            "Fetching 9 files:  33% 3/9 [00:02<00:06,  1.01s/it]\n",
            "videos/observation.images.top/chunk-000/(…):   2% 7.51M/320M [00:03<02:44, 1.91MB/s]\u001b[A\n",
            "videos/observation.images.top/chunk-000/(…):   5% 17.3M/320M [00:04<00:58, 5.20MB/s]\u001b[A\n",
            "videos/observation.images.top/chunk-000/(…):   7% 22.6M/320M [00:04<00:40, 7.39MB/s]\u001b[A\n",
            "videos/observation.images.top/chunk-000/(…):  16% 52.1M/320M [00:04<00:11, 23.9MB/s]\u001b[A\n",
            "\n",
            "videos/observation.images.wrist/chunk-00(…):  10% 15.0M/149M [00:04<00:42, 3.15MB/s]\u001b[A\u001b[A\n",
            "videos/observation.images.top/chunk-000/(…):  37% 119M/320M [00:08<00:11, 17.9MB/s] \u001b[A\n",
            "videos/observation.images.top/chunk-000/(…):  79% 253M/320M [00:08<00:01, 48.0MB/s]\u001b[A\n",
            "videos/observation.images.top/chunk-000/(…): 100% 320M/320M [00:09<00:00, 34.7MB/s]\n",
            "Fetching 9 files:  89% 8/9 [00:09<00:01,  1.26s/it]\n",
            "\n",
            "videos/observation.images.wrist/chunk-00(…):  55% 82.1M/149M [00:09<00:06, 9.95MB/s]\u001b[A\u001b[A\n",
            "\n",
            "videos/observation.images.wrist/chunk-00(…): 100% 149M/149M [00:09<00:00, 15.7MB/s]\n",
            "Fetching 9 files: 100% 9/9 [00:09<00:00,  1.11s/it]\n",
            "INFO 2025-12-22 04:23:39 ot_train.py:202 Creating policy\n",
            "Loading  HuggingFaceTB/SmolVLM2-500M-Video-Instruct weights ...\n",
            "config.json: 3.77kB [00:00, 11.4MB/s]\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "model.safetensors: 100% 2.03G/2.03G [00:10<00:00, 198MB/s]\n",
            "INFO 2025-12-22 04:23:52 modeling.py:987 We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
            "generation_config.json: 100% 136/136 [00:00<00:00, 1.00MB/s]\n",
            "processor_config.json: 100% 67.0/67.0 [00:00<00:00, 477kB/s]\n",
            "chat_template.json: 100% 430/430 [00:00<00:00, 3.01MB/s]\n",
            "preprocessor_config.json: 100% 599/599 [00:00<00:00, 4.19MB/s]\n",
            "tokenizer_config.json: 28.6kB [00:00, 65.5MB/s]\n",
            "vocab.json: 801kB [00:00, 8.24MB/s]\n",
            "merges.txt: 466kB [00:00, 10.1MB/s]\n",
            "tokenizer.json: 3.55MB [00:00, 27.1MB/s]\n",
            "added_tokens.json: 4.74kB [00:00, 16.7MB/s]\n",
            "special_tokens_map.json: 100% 868/868 [00:00<00:00, 6.67MB/s]\n",
            "Reducing the number of VLM layers to 16 ...\n",
            "model.safetensors: 100% 907M/907M [00:10<00:00, 85.3MB/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/lerobot-train\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lerobot/scripts/lerobot_train.py\", line 449, in main\n",
            "    train()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lerobot/configs/parser.py\", line 233, in wrapper_inner\n",
            "    response = fn(cfg, *args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lerobot/scripts/lerobot_train.py\", line 203, in train\n",
            "    policy = make_policy(\n",
            "             ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lerobot/policies/factory.py\", line 424, in make_policy\n",
            "    validate_visual_features_consistency(cfg, features)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lerobot/policies/utils.py\", line 241, in validate_visual_features_consistency\n",
            "    raise_feature_mismatch_error(provided_visuals, expected_visuals)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/lerobot/policies/utils.py\", line 215, in raise_feature_mismatch_error\n",
            "    raise ValueError(\n",
            "ValueError: Feature mismatch between dataset/environment and policy config.\n",
            "- Missing features: ['observation.images.camera1', 'observation.images.camera2', 'observation.images.camera3']\n",
            "- Extra features: ['observation.images.top', 'observation.images.wrist']\n",
            "\n",
            "Please ensure your dataset and policy use consistent feature names.\n",
            "If your dataset uses different observation keys (e.g., cameras named differently), use the `--rename_map` argument, for example:\n",
            "  --rename_map='{\"observation.images.left\": \"observation.images.camera1\", \"observation.images.top\": \"observation.images.camera2\"}'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lerobot-train \\\n",
        "  --policy.path=lerobot/smolvla_base \\\n",
        "  --policy.repo_id=Sythen/my_smolvla_training_fm \\\n",
        "  --dataset.repo_id=lerobot/svla_so100_pickplace \\\n",
        "  --rename_map='{\"observation.images.top\":\"observation.images.camera1\",\"observation.images.wrist\":\"observation.images.camera2\"}' \\\n",
        "  --batch_size=8 \\\n",
        "  --steps=200 \\\n",
        "  --output_dir=/content/outputs/train/my_smolvla \\\n",
        "  --job_name=my_smolvla_training_test \\\n",
        "  --policy.device=cuda \\\n",
        "  --wandb.enable=false\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C27KB3nDTeD1",
        "outputId": "f0b6aa9c-d0dc-4871-dcaf-7e9a20032ca4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-22 04:25:22.546523: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766377522.570104    6776 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766377522.577094    6776 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766377522.595280    6776 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766377522.595319    6776 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766377522.595324    6776 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766377522.595329    6776 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-22 04:25:22.600690: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 2025-12-22 04:25:33 ot_train.py:163 {'batch_size': 8,\n",
            " 'checkpoint_path': None,\n",
            " 'dataset': {'episodes': None,\n",
            "             'image_transforms': {'enable': False,\n",
            "                                  'max_num_transforms': 3,\n",
            "                                  'random_order': False,\n",
            "                                  'tfs': {'affine': {'kwargs': {'degrees': [-5.0,\n",
            "                                                                            5.0],\n",
            "                                                                'translate': [0.05,\n",
            "                                                                              0.05]},\n",
            "                                                     'type': 'RandomAffine',\n",
            "                                                     'weight': 1.0},\n",
            "                                          'brightness': {'kwargs': {'brightness': [0.8,\n",
            "                                                                                   1.2]},\n",
            "                                                         'type': 'ColorJitter',\n",
            "                                                         'weight': 1.0},\n",
            "                                          'contrast': {'kwargs': {'contrast': [0.8,\n",
            "                                                                               1.2]},\n",
            "                                                       'type': 'ColorJitter',\n",
            "                                                       'weight': 1.0},\n",
            "                                          'hue': {'kwargs': {'hue': [-0.05,\n",
            "                                                                     0.05]},\n",
            "                                                  'type': 'ColorJitter',\n",
            "                                                  'weight': 1.0},\n",
            "                                          'saturation': {'kwargs': {'saturation': [0.5,\n",
            "                                                                                   1.5]},\n",
            "                                                         'type': 'ColorJitter',\n",
            "                                                         'weight': 1.0},\n",
            "                                          'sharpness': {'kwargs': {'sharpness': [0.5,\n",
            "                                                                                 1.5]},\n",
            "                                                        'type': 'SharpnessJitter',\n",
            "                                                        'weight': 1.0}}},\n",
            "             'repo_id': 'lerobot/svla_so100_pickplace',\n",
            "             'revision': None,\n",
            "             'root': None,\n",
            "             'streaming': False,\n",
            "             'use_imagenet_stats': True,\n",
            "             'video_backend': 'torchcodec'},\n",
            " 'env': None,\n",
            " 'eval': {'batch_size': 50, 'n_episodes': 50, 'use_async_envs': False},\n",
            " 'eval_freq': 20000,\n",
            " 'job_name': 'my_smolvla_training_test',\n",
            " 'log_freq': 200,\n",
            " 'num_workers': 4,\n",
            " 'optimizer': {'betas': [0.9, 0.95],\n",
            "               'eps': 1e-08,\n",
            "               'grad_clip_norm': 10.0,\n",
            "               'lr': 0.0001,\n",
            "               'type': 'adamw',\n",
            "               'weight_decay': 1e-10},\n",
            " 'output_dir': '/content/outputs/train/my_smolvla',\n",
            " 'policy': {'adapt_to_pi_aloha': False,\n",
            "            'add_image_special_tokens': False,\n",
            "            'attention_mode': 'cross_attn',\n",
            "            'chunk_size': 50,\n",
            "            'device': 'cuda',\n",
            "            'empty_cameras': 0,\n",
            "            'expert_width_multiplier': 0.75,\n",
            "            'freeze_vision_encoder': True,\n",
            "            'input_features': {'observation.images.camera1': {'shape': [3,\n",
            "                                                                        256,\n",
            "                                                                        256],\n",
            "                                                              'type': <FeatureType.VISUAL: 'VISUAL'>},\n",
            "                               'observation.images.camera2': {'shape': [3,\n",
            "                                                                        256,\n",
            "                                                                        256],\n",
            "                                                              'type': <FeatureType.VISUAL: 'VISUAL'>},\n",
            "                               'observation.images.camera3': {'shape': [3,\n",
            "                                                                        256,\n",
            "                                                                        256],\n",
            "                                                              'type': <FeatureType.VISUAL: 'VISUAL'>},\n",
            "                               'observation.state': {'shape': [6],\n",
            "                                                     'type': <FeatureType.STATE: 'STATE'>}},\n",
            "            'license': None,\n",
            "            'load_vlm_weights': True,\n",
            "            'max_action_dim': 32,\n",
            "            'max_period': 4.0,\n",
            "            'max_state_dim': 32,\n",
            "            'min_period': 0.004,\n",
            "            'n_action_steps': 50,\n",
            "            'n_obs_steps': 1,\n",
            "            'normalization_mapping': {'ACTION': <NormalizationMode.MEAN_STD: 'MEAN_STD'>,\n",
            "                                      'STATE': <NormalizationMode.MEAN_STD: 'MEAN_STD'>,\n",
            "                                      'VISUAL': <NormalizationMode.IDENTITY: 'IDENTITY'>},\n",
            "            'num_expert_layers': 0,\n",
            "            'num_steps': 10,\n",
            "            'num_vlm_layers': 16,\n",
            "            'optimizer_betas': [0.9, 0.95],\n",
            "            'optimizer_eps': 1e-08,\n",
            "            'optimizer_grad_clip_norm': 10.0,\n",
            "            'optimizer_lr': 0.0001,\n",
            "            'optimizer_weight_decay': 1e-10,\n",
            "            'output_features': {'action': {'shape': [6],\n",
            "                                           'type': <FeatureType.ACTION: 'ACTION'>}},\n",
            "            'pad_language_to': 'max_length',\n",
            "            'prefix_length': 0,\n",
            "            'pretrained_path': 'lerobot/smolvla_base',\n",
            "            'private': None,\n",
            "            'push_to_hub': True,\n",
            "            'repo_id': 'Sythen/my_smolvla_training_fm',\n",
            "            'resize_imgs_with_padding': [512, 512],\n",
            "            'rtc_config': None,\n",
            "            'scheduler_decay_lr': 2.5e-06,\n",
            "            'scheduler_decay_steps': 30000,\n",
            "            'scheduler_warmup_steps': 1000,\n",
            "            'self_attn_every_n_layers': 2,\n",
            "            'tags': None,\n",
            "            'tokenizer_max_length': 48,\n",
            "            'train_expert_only': True,\n",
            "            'train_state_proj': True,\n",
            "            'type': 'smolvla',\n",
            "            'use_amp': False,\n",
            "            'use_cache': True,\n",
            "            'use_delta_joint_actions_aloha': False,\n",
            "            'vlm_model_name': 'HuggingFaceTB/SmolVLM2-500M-Video-Instruct'},\n",
            " 'rename_map': {'observation.images.top': 'observation.images.camera1',\n",
            "                'observation.images.wrist': 'observation.images.camera2'},\n",
            " 'resume': False,\n",
            " 'save_checkpoint': True,\n",
            " 'save_freq': 20000,\n",
            " 'scheduler': {'decay_lr': 2.5e-06,\n",
            "               'num_decay_steps': 30000,\n",
            "               'num_warmup_steps': 1000,\n",
            "               'peak_lr': 0.0001,\n",
            "               'type': 'cosine_decay_with_warmup'},\n",
            " 'seed': 1000,\n",
            " 'steps': 200,\n",
            " 'use_policy_training_preset': True,\n",
            " 'wandb': {'disable_artifact': False,\n",
            "           'enable': False,\n",
            "           'entity': None,\n",
            "           'mode': None,\n",
            "           'notes': None,\n",
            "           'project': 'lerobot',\n",
            "           'run_id': None}}\n",
            "INFO 2025-12-22 04:25:33 ot_train.py:171 \u001b[1m\u001b[33mLogs will be saved locally.\u001b[0m\n",
            "INFO 2025-12-22 04:25:33 ot_train.py:183 Creating dataset\n",
            "INFO 2025-12-22 04:25:33 ot_train.py:202 Creating policy\n",
            "Loading  HuggingFaceTB/SmolVLM2-500M-Video-Instruct weights ...\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 2025-12-22 04:25:34 modeling.py:987 We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
            "Reducing the number of VLM layers to 16 ...\n",
            "policy_preprocessor.json: 1.87kB [00:00, 7.90MB/s]\n",
            "policy_preprocessor_step_5_normalizer_pr(…): 100% 640/640 [00:01<00:00, 451B/s]\n",
            "policy_postprocessor.json: 100% 660/660 [00:00<00:00, 4.52MB/s]\n",
            "INFO 2025-12-22 04:25:53 ot_train.py:247 Creating optimizer and scheduler\n",
            "INFO 2025-12-22 04:25:53 hedulers.py:105 Auto-scaling LR scheduler: num_training_steps (200) < num_decay_steps (30000). Scaling warmup: 1000 → 6, decay: 30000 → 200 (scale factor: 0.007)\n",
            "INFO 2025-12-22 04:25:53 ot_train.py:259 \u001b[1m\u001b[33mOutput dir:\u001b[0m /content/outputs/train/my_smolvla\n",
            "INFO 2025-12-22 04:25:53 ot_train.py:264 cfg.steps=200 (200)\n",
            "INFO 2025-12-22 04:25:53 ot_train.py:265 dataset.num_frames=19631 (20K)\n",
            "INFO 2025-12-22 04:25:53 ot_train.py:266 dataset.num_episodes=50\n",
            "INFO 2025-12-22 04:25:53 ot_train.py:269 Effective batch size: 8 x 1 = 8\n",
            "INFO 2025-12-22 04:25:53 ot_train.py:270 num_learnable_params=99880992 (100M)\n",
            "INFO 2025-12-22 04:25:53 ot_train.py:271 num_total_params=450046176 (450M)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "INFO 2025-12-22 04:25:53 ot_train.py:327 Start offline training on a fixed dataset\n",
            "INFO 2025-12-22 04:33:41 ot_train.py:354 step:200 smpl:2K ep:4 epch:0.08 loss:0.060 grdn:0.978 lr:5.0e-05 updt_s:2.310 data_s:0.028\n",
            "INFO 2025-12-22 04:33:41 ot_train.py:364 Checkpoint policy after step 200\n",
            "INFO 2025-12-22 04:34:38 ot_train.py:435 End of training\n",
            "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            \n",
            "New Data Upload               : |          |  0.00B /  0.00B            \u001b[A\n",
            "\n",
            "  ...ning_fm/model.safetensors:   0% 609k/907M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :   0% 609k/907M [00:02<54:54, 275kB/s,  304kB/s  ]\n",
            "New Data Upload               :   1% 609k/67.1M [00:02<04:01, 275kB/s,  304kB/s  ]\u001b[A\n",
            "\n",
            "  ...ning_fm/model.safetensors:   0% 609k/907M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "  ...ning_fm/model.safetensors:   0% 609k/907M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :   0% 1.22M/907M [00:02<31:19, 482kB/s,  468kB/s  ]\n",
            "New Data Upload               :   2% 1.22M/67.1M [00:02<02:16, 482kB/s,  468kB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :   0% 2.44M/907M [00:03<13:13, 1.14MB/s,  870kB/s  ]\n",
            "New Data Upload               :   2% 2.44M/134M [00:03<01:55, 1.14MB/s,  870kB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :   0% 3.65M/907M [00:03<08:09, 1.85MB/s, 1.22MB/s  ]\n",
            "New Data Upload               :   3% 3.65M/134M [00:03<01:10, 1.85MB/s, 1.22MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :   1% 6.09M/907M [00:03<04:09, 3.61MB/s, 1.90MB/s  ]\n",
            "New Data Upload               :   5% 6.09M/134M [00:03<00:35, 3.61MB/s, 1.90MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :   1% 6.71M/907M [00:03<04:16, 3.51MB/s, 1.97MB/s  ]\n",
            "New Data Upload               :   5% 6.71M/134M [00:03<00:36, 3.51MB/s, 1.97MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :   1% 8.54M/907M [00:03<03:09, 4.74MB/s, 2.37MB/s  ]\n",
            "New Data Upload               :   6% 8.54M/134M [00:03<00:26, 4.74MB/s, 2.37MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :   1% 12.2M/907M [00:04<01:52, 7.96MB/s, 3.21MB/s  ]\n",
            "New Data Upload               :   9% 12.2M/134M [00:04<00:15, 7.96MB/s, 3.21MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :   2% 15.8M/907M [00:04<01:24, 10.6MB/s, 3.96MB/s  ]\n",
            "New Data Upload               :  12% 15.8M/134M [00:04<00:11, 10.6MB/s, 3.96MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :   2% 20.7M/907M [00:04<01:02, 14.3MB/s, 4.93MB/s  ]\n",
            "New Data Upload               :  10% 20.7M/201M [00:04<00:12, 14.3MB/s, 4.93MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :   3% 25.6M/907M [00:04<00:51, 17.1MB/s, 5.82MB/s  ]\n",
            "New Data Upload               :  13% 25.6M/201M [00:04<00:10, 17.1MB/s, 5.82MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :   3% 31.1M/907M [00:04<00:43, 20.0MB/s, 6.77MB/s  ]\n",
            "New Data Upload               :  15% 31.1M/201M [00:04<00:08, 20.0MB/s, 6.77MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :   4% 37.3M/907M [00:05<00:37, 23.1MB/s, 7.77MB/s  ]\n",
            "New Data Upload               :  19% 37.3M/201M [00:05<00:07, 23.1MB/s, 7.77MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :   5% 42.8M/907M [00:05<00:35, 24.4MB/s, 8.56MB/s  ]\n",
            "New Data Upload               :  21% 42.8M/201M [00:05<00:06, 24.4MB/s, 8.56MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :   5% 47.1M/907M [00:05<00:36, 23.5MB/s, 9.06MB/s  ]\n",
            "New Data Upload               :  23% 47.1M/201M [00:05<00:06, 23.5MB/s, 9.06MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :   6% 51.4M/907M [00:05<00:37, 23.0MB/s, 9.52MB/s  ]\n",
            "New Data Upload               :  26% 51.4M/201M [00:05<00:06, 23.0MB/s, 9.52MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :   7% 59.4M/907M [00:05<00:30, 28.1MB/s, 10.6MB/s  ]\n",
            "New Data Upload               :  30% 59.4M/201M [00:05<00:05, 28.1MB/s, 10.6MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :   8% 70.6M/907M [00:06<00:23, 36.3MB/s, 12.2MB/s  ]\n",
            "New Data Upload               :  35% 70.6M/201M [00:06<00:03, 36.3MB/s, 12.2MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :   9% 78.0M/907M [00:06<00:22, 36.5MB/s, 13.0MB/s  ]\n",
            "New Data Upload               :  39% 78.0M/201M [00:06<00:03, 36.5MB/s, 13.0MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  10% 86.6M/907M [00:06<00:21, 38.5MB/s, 14.0MB/s  ]\n",
            "New Data Upload               :  43% 86.6M/201M [00:06<00:02, 38.5MB/s, 14.0MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  11% 97.7M/907M [00:06<00:18, 43.6MB/s, 15.3MB/s  ]\n",
            "New Data Upload               :  49% 97.7M/201M [00:06<00:02, 43.6MB/s, 15.3MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  12% 107M/907M [00:06<00:18, 44.4MB/s, 16.2MB/s  ] \n",
            "New Data Upload               :  53% 107M/201M [00:06<00:02, 44.4MB/s, 16.2MB/s  ] \u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  18% 164M/907M [00:07<00:06, 116MB/s, 24.1MB/s  ] \n",
            "New Data Upload               :  57% 114M/201M [00:07<00:02, 41.1MB/s, 16.7MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  22% 197M/907M [00:07<00:05, 131MB/s, 28.1MB/s  ]\n",
            "New Data Upload               :  60% 121M/201M [00:07<00:01, 40.0MB/s, 17.3MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  27% 241M/907M [00:07<00:04, 159MB/s, 33.5MB/s  ]\n",
            "New Data Upload               :  66% 132M/201M [00:07<00:01, 44.8MB/s, 18.4MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  31% 284M/907M [00:07<00:03, 174MB/s, 38.3MB/s  ]\n",
            "New Data Upload               :  70% 141M/201M [00:07<00:01, 44.3MB/s, 19.1MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  35% 318M/907M [00:07<00:03, 173MB/s, 41.8MB/s  ]\n",
            "New Data Upload               :  74% 150M/201M [00:07<00:01, 44.1MB/s, 19.7MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  40% 360M/907M [00:08<00:02, 185MB/s, 46.1MB/s  ]\n",
            "New Data Upload               :  79% 158M/201M [00:08<00:00, 44.0MB/s, 20.3MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  44% 402M/907M [00:08<00:02, 192MB/s, 50.2MB/s  ]\n",
            "New Data Upload               :  83% 167M/201M [00:08<00:00, 43.0MB/s, 20.8MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  48% 439M/907M [00:08<00:02, 190MB/s, 53.5MB/s  ]\n",
            "New Data Upload               :  85% 170M/201M [00:08<00:00, 35.7MB/s, 20.8MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  52% 470M/907M [00:08<00:02, 179MB/s, 55.9MB/s  ]\n",
            "New Data Upload               :  87% 176M/201M [00:08<00:00, 33.4MB/s, 20.9MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  56% 507M/907M [00:08<00:02, 182MB/s, 59.0MB/s  ]\n",
            "New Data Upload               :  90% 180M/201M [00:08<00:00, 29.9MB/s, 21.0MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  60% 545M/907M [00:09<00:01, 185MB/s, 62.0MB/s  ]\n",
            "New Data Upload               :  92% 185M/201M [00:09<00:00, 27.5MB/s, 21.0MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  64% 583M/907M [00:09<00:01, 186MB/s, 64.8MB/s  ]\n",
            "New Data Upload               :  94% 189M/201M [00:09<00:00, 25.8MB/s, 21.0MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  68% 621M/907M [00:09<00:01, 186MB/s, 67.4MB/s  ]\n",
            "New Data Upload               :  96% 193M/201M [00:09<00:00, 23.6MB/s, 20.9MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  73% 659M/907M [00:09<00:01, 187MB/s, 70.1MB/s  ]\n",
            "New Data Upload               :  98% 198M/201M [00:09<00:00, 23.8MB/s, 21.0MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  77% 696M/907M [00:09<00:01, 186MB/s, 72.4MB/s  ]\n",
            "New Data Upload               : 100% 201M/201M [00:09<00:00, 21.3MB/s, 20.9MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  80% 729M/907M [00:10<00:00, 181MB/s, 74.4MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  84% 763M/907M [00:10<00:00, 177MB/s, 76.3MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  88% 796M/907M [00:10<00:00, 174MB/s, 78.1MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  91% 821M/907M [00:10<00:00, 160MB/s, 80.5MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  95% 864M/907M [00:10<00:00, 175MB/s, 84.7MB/s  ]\n",
            "New Data Upload               : 100% 201M/201M [00:10<00:00, 7.17MB/s, 19.7MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  96% 873M/907M [00:11<00:00, 137MB/s, 85.6MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  97% 877M/907M [00:11<00:00, 102MB/s, 86.0MB/s  ]\n",
            "New Data Upload               :  88% 205M/235M [00:11<00:03, 8.16MB/s, 20.1MB/s  ]\u001b[A\n",
            "\n",
            "  ...ning_fm/model.safetensors:  97% 877M/907M [00:09<00:00, 95.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  97% 881M/907M [00:11<00:00, 59.3MB/s, 86.4MB/s  ]\n",
            "New Data Upload               :  89% 209M/235M [00:11<00:03, 8.44MB/s, 20.5MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  98% 885M/907M [00:11<00:00, 49.1MB/s, 86.7MB/s  ]\n",
            "New Data Upload               :  91% 213M/235M [00:11<00:02, 10.1MB/s, 20.9MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  98% 888M/907M [00:12<00:00, 40.9MB/s, 87.1MB/s  ]\n",
            "New Data Upload               :  92% 216M/235M [00:12<00:01, 11.7MB/s, 21.2MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  98% 892M/907M [00:12<00:00, 34.8MB/s, 87.4MB/s  ]\n",
            "New Data Upload               :  94% 220M/235M [00:12<00:01, 13.1MB/s, 21.6MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  99% 896M/907M [00:12<00:00, 30.2MB/s, 87.7MB/s  ]\n",
            "New Data Upload               :  95% 224M/235M [00:12<00:00, 14.3MB/s, 21.9MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :  99% 899M/907M [00:12<00:00, 26.7MB/s, 88.1MB/s  ]\n",
            "New Data Upload               :  97% 227M/235M [00:12<00:00, 15.3MB/s, 22.2MB/s  ]\u001b[A\n",
            "\n",
            "  ...ning_fm/model.safetensors:  99% 899M/907M [00:10<00:00, 84.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      : 100% 903M/907M [00:13<00:00, 18.8MB/s, 88.4MB/s  ]\n",
            "New Data Upload               :  98% 231M/235M [00:13<00:00, 12.7MB/s, 22.5MB/s  ]\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      : 100% 907M/907M [00:13<00:00, 18.7MB/s, 88.6MB/s  ]\n",
            "New Data Upload               : 100% 235M/235M [00:13<00:00, 14.0MB/s, 22.8MB/s  ]\u001b[A\n",
            "\n",
            "  ...ning_fm/model.safetensors: 100% 907M/907M [00:11<00:00, 80.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing Files (1 / 1)      : 100% 907M/907M [00:13<00:00, 11.2MB/s, 88.3MB/s  ]\n",
            "New Data Upload               : 100% 235M/235M [00:13<00:00, 8.58MB/s, 22.4MB/s  ]\u001b[A\n",
            "\n",
            "  ...ning_fm/model.safetensors: 100% 907M/907M [00:11<00:00, 78.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "  ...ning_fm/model.safetensors: 100% 907M/907M [00:11<00:00, 76.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "  ...ning_fm/model.safetensors: 100% 907M/907M [00:11<00:00, 75.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing Files (1 / 1)      : 100% 907M/907M [00:14<00:00, 63.8MB/s, 89.1MB/s  ]\n",
            "New Data Upload               : 100% 235M/235M [00:14<00:00, 16.5MB/s, 21.9MB/s  ]\n",
            "  ...ning_fm/model.safetensors: 100% 907M/907M [00:12<00:00, 75.5MB/s]\n",
            "INFO 2025-12-22 04:36:03 etrained.py:237 Model pushed to https://huggingface.co/Sythen/my_smolvla_training_fm\n",
            "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            \n",
            "New Data Upload               : |          |  0.00B /  0.00B            \u001b[A\n",
            "\n",
            "  ...zer_processor.safetensors: 100% 3.75k/3.75k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing Files (1 / 1)      : 100% 3.75k/3.75k [00:00<00:00, 4.66kB/s, 6.24kB/s  ]\n",
            "\n",
            "  ...zer_processor.safetensors: 100% 3.75k/3.75k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing Files (1 / 1)      : 100% 3.75k/3.75k [00:01<00:00, 3.73kB/s, 4.68kB/s  ]\n",
            "New Data Upload               : |          |  0.00B /  0.00B,  0.00B/s  \n",
            "  ...zer_processor.safetensors: 100% 3.75k/3.75k [00:00<?, ?B/s]\n",
            "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            \n",
            "New Data Upload               : |          |  0.00B /  0.00B            \u001b[A\n",
            "\n",
            "  ...zer_processor.safetensors: 100% 3.75k/3.75k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing Files (1 / 1)      : 100% 3.75k/3.75k [00:00<00:00, 18.5kB/s,   ???B/s  ]\n",
            "\n",
            "  ...zer_processor.safetensors: 100% 3.75k/3.75k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing Files (1 / 1)      : 100% 3.75k/3.75k [00:00<00:00, 9.31kB/s,  0.00B/s  ]\n",
            "New Data Upload               : |          |  0.00B /  0.00B,  0.00B/s  \n",
            "  ...zer_processor.safetensors: 100% 3.75k/3.75k [00:00<?, ?B/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -R /content/outputs | head -n 200\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kf2Eajx_akwh",
        "outputId": "40a0c56a-c905-4365-ffe5-102d41fe4e08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/outputs:\n",
            "train\n",
            "\n",
            "/content/outputs/train:\n",
            "my_smolvla\n",
            "\n",
            "/content/outputs/train/my_smolvla:\n",
            "checkpoints\n",
            "\n",
            "/content/outputs/train/my_smolvla/checkpoints:\n",
            "000200\n",
            "last\n",
            "\n",
            "/content/outputs/train/my_smolvla/checkpoints/000200:\n",
            "pretrained_model\n",
            "training_state\n",
            "\n",
            "/content/outputs/train/my_smolvla/checkpoints/000200/pretrained_model:\n",
            "config.json\n",
            "model.safetensors\n",
            "policy_postprocessor.json\n",
            "policy_postprocessor_step_0_unnormalizer_processor.safetensors\n",
            "policy_preprocessor.json\n",
            "policy_preprocessor_step_5_normalizer_processor.safetensors\n",
            "train_config.json\n",
            "\n",
            "/content/outputs/train/my_smolvla/checkpoints/000200/training_state:\n",
            "optimizer_param_groups.json\n",
            "optimizer_state.safetensors\n",
            "rng_state.safetensors\n",
            "scheduler_state.json\n",
            "training_step.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/outputs/train/my_smolvla/checkpoints/000200/pretrained_model/train_config.json \\\n",
        "    /content/outputs/train/my_smolvla/checkpoints/000200/pretrained_model/train_config_l1.json\n"
      ],
      "metadata": {
        "id": "nqG6fxEVa9wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "src = \"/content/outputs/train/my_smolvla/checkpoints/000200/pretrained_model/train_config_l1.json\"\n",
        "\n",
        "with open(src, \"r\") as f:\n",
        "    cfg = json.load(f)\n",
        "\n",
        "# Change prediction_type anywhere it appears\n",
        "def set_prediction_type(obj):\n",
        "    if isinstance(obj, dict):\n",
        "        for k in list(obj.keys()):\n",
        "            if k == \"prediction_type\":\n",
        "                obj[k] = \"sample\"   # L1 regression mode\n",
        "            else:\n",
        "                set_prediction_type(obj[k])\n",
        "    elif isinstance(obj, list):\n",
        "        for x in obj:\n",
        "            set_prediction_type(x)\n",
        "\n",
        "set_prediction_type(cfg)\n",
        "\n",
        "with open(src, \"w\") as f:\n",
        "    json.dump(cfg, f, indent=2)\n",
        "\n",
        "print(\"✅ Updated prediction_type to sample in:\", src)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gP6lvPdebCTg",
        "outputId": "2b3e00cc-5f89-4e62-d2d7-1613b503b50e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Updated prediction_type to sample in: /content/outputs/train/my_smolvla/checkpoints/000200/pretrained_model/train_config_l1.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lerobot-train \\\n",
        "  --config_path /content/outputs/train/my_smolvla/checkpoints/000200/pretrained_model/train_config_l1.json \\\n",
        "  --policy.repo_id Sythen/my_smolvla_training_l1 \\\n",
        "  --policy.num_steps 1000 \\\n",
        "  --policy.device cuda \\\n",
        "  --policy.push_to_hub true \\\n",
        "  --output_dir /content/outputs/train/my_smolvla_l1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4On7xENNbE7m",
        "outputId": "e5fea5c5-ef73-4cf9-f34c-fb555e84555f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-22 05:02:30.928625: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766379750.949696   16218 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766379750.956092   16218 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766379750.972821   16218 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766379750.972853   16218 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766379750.972857   16218 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766379750.972862   16218 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-22 05:02:30.978220: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 2025-12-22 05:02:40 ot_train.py:163 {'batch_size': 8,\n",
            " 'checkpoint_path': None,\n",
            " 'dataset': {'episodes': None,\n",
            "             'image_transforms': {'enable': False,\n",
            "                                  'max_num_transforms': 3,\n",
            "                                  'random_order': False,\n",
            "                                  'tfs': {'affine': {'kwargs': {'degrees': [-5.0,\n",
            "                                                                            5.0],\n",
            "                                                                'translate': [0.05,\n",
            "                                                                              0.05]},\n",
            "                                                     'type': 'RandomAffine',\n",
            "                                                     'weight': 1.0},\n",
            "                                          'brightness': {'kwargs': {'brightness': [0.8,\n",
            "                                                                                   1.2]},\n",
            "                                                         'type': 'ColorJitter',\n",
            "                                                         'weight': 1.0},\n",
            "                                          'contrast': {'kwargs': {'contrast': [0.8,\n",
            "                                                                               1.2]},\n",
            "                                                       'type': 'ColorJitter',\n",
            "                                                       'weight': 1.0},\n",
            "                                          'hue': {'kwargs': {'hue': [-0.05,\n",
            "                                                                     0.05]},\n",
            "                                                  'type': 'ColorJitter',\n",
            "                                                  'weight': 1.0},\n",
            "                                          'saturation': {'kwargs': {'saturation': [0.5,\n",
            "                                                                                   1.5]},\n",
            "                                                         'type': 'ColorJitter',\n",
            "                                                         'weight': 1.0},\n",
            "                                          'sharpness': {'kwargs': {'sharpness': [0.5,\n",
            "                                                                                 1.5]},\n",
            "                                                        'type': 'SharpnessJitter',\n",
            "                                                        'weight': 1.0}}},\n",
            "             'repo_id': 'lerobot/svla_so100_pickplace',\n",
            "             'revision': None,\n",
            "             'root': None,\n",
            "             'streaming': False,\n",
            "             'use_imagenet_stats': True,\n",
            "             'video_backend': 'torchcodec'},\n",
            " 'env': None,\n",
            " 'eval': {'batch_size': 50, 'n_episodes': 50, 'use_async_envs': False},\n",
            " 'eval_freq': 20000,\n",
            " 'job_name': 'my_smolvla_training_test',\n",
            " 'log_freq': 200,\n",
            " 'num_workers': 4,\n",
            " 'optimizer': {'betas': [0.9, 0.95],\n",
            "               'eps': 1e-08,\n",
            "               'grad_clip_norm': 10.0,\n",
            "               'lr': 0.0001,\n",
            "               'type': 'adamw',\n",
            "               'weight_decay': 1e-10},\n",
            " 'output_dir': '/content/outputs/train/my_smolvla_l1',\n",
            " 'policy': {'adapt_to_pi_aloha': False,\n",
            "            'add_image_special_tokens': False,\n",
            "            'attention_mode': 'cross_attn',\n",
            "            'chunk_size': 50,\n",
            "            'device': 'cuda',\n",
            "            'empty_cameras': 0,\n",
            "            'expert_width_multiplier': 0.75,\n",
            "            'freeze_vision_encoder': True,\n",
            "            'input_features': {'observation.images.camera1': {'shape': [3,\n",
            "                                                                        256,\n",
            "                                                                        256],\n",
            "                                                              'type': <FeatureType.VISUAL: 'VISUAL'>},\n",
            "                               'observation.images.camera2': {'shape': [3,\n",
            "                                                                        256,\n",
            "                                                                        256],\n",
            "                                                              'type': <FeatureType.VISUAL: 'VISUAL'>},\n",
            "                               'observation.images.camera3': {'shape': [3,\n",
            "                                                                        256,\n",
            "                                                                        256],\n",
            "                                                              'type': <FeatureType.VISUAL: 'VISUAL'>},\n",
            "                               'observation.state': {'shape': [6],\n",
            "                                                     'type': <FeatureType.STATE: 'STATE'>}},\n",
            "            'license': None,\n",
            "            'load_vlm_weights': True,\n",
            "            'max_action_dim': 32,\n",
            "            'max_period': 4.0,\n",
            "            'max_state_dim': 32,\n",
            "            'min_period': 0.004,\n",
            "            'n_action_steps': 50,\n",
            "            'n_obs_steps': 1,\n",
            "            'normalization_mapping': {'ACTION': <NormalizationMode.MEAN_STD: 'MEAN_STD'>,\n",
            "                                      'STATE': <NormalizationMode.MEAN_STD: 'MEAN_STD'>,\n",
            "                                      'VISUAL': <NormalizationMode.IDENTITY: 'IDENTITY'>},\n",
            "            'num_expert_layers': 0,\n",
            "            'num_steps': 1000,\n",
            "            'num_vlm_layers': 16,\n",
            "            'optimizer_betas': [0.9, 0.95],\n",
            "            'optimizer_eps': 1e-08,\n",
            "            'optimizer_grad_clip_norm': 10.0,\n",
            "            'optimizer_lr': 0.0001,\n",
            "            'optimizer_weight_decay': 1e-10,\n",
            "            'output_features': {'action': {'shape': [6],\n",
            "                                           'type': <FeatureType.ACTION: 'ACTION'>}},\n",
            "            'pad_language_to': 'max_length',\n",
            "            'prefix_length': 0,\n",
            "            'pretrained_path': 'lerobot/smolvla_base',\n",
            "            'private': None,\n",
            "            'push_to_hub': True,\n",
            "            'repo_id': 'Sythen/my_smolvla_training_l1',\n",
            "            'resize_imgs_with_padding': [512, 512],\n",
            "            'rtc_config': None,\n",
            "            'scheduler_decay_lr': 2.5e-06,\n",
            "            'scheduler_decay_steps': 30000,\n",
            "            'scheduler_warmup_steps': 1000,\n",
            "            'self_attn_every_n_layers': 2,\n",
            "            'tags': None,\n",
            "            'tokenizer_max_length': 48,\n",
            "            'train_expert_only': True,\n",
            "            'train_state_proj': True,\n",
            "            'type': 'smolvla',\n",
            "            'use_amp': False,\n",
            "            'use_cache': True,\n",
            "            'use_delta_joint_actions_aloha': False,\n",
            "            'vlm_model_name': 'HuggingFaceTB/SmolVLM2-500M-Video-Instruct'},\n",
            " 'rename_map': {'observation.images.top': 'observation.images.camera1',\n",
            "                'observation.images.wrist': 'observation.images.camera2'},\n",
            " 'resume': False,\n",
            " 'save_checkpoint': True,\n",
            " 'save_freq': 20000,\n",
            " 'scheduler': {'decay_lr': 2.5e-06,\n",
            "               'num_decay_steps': 30000,\n",
            "               'num_warmup_steps': 1000,\n",
            "               'peak_lr': 0.0001,\n",
            "               'type': 'cosine_decay_with_warmup'},\n",
            " 'seed': 1000,\n",
            " 'steps': 200,\n",
            " 'use_policy_training_preset': True,\n",
            " 'wandb': {'disable_artifact': False,\n",
            "           'enable': False,\n",
            "           'entity': None,\n",
            "           'mode': None,\n",
            "           'notes': None,\n",
            "           'project': 'lerobot',\n",
            "           'run_id': None}}\n",
            "INFO 2025-12-22 05:02:40 ot_train.py:171 \u001b[1m\u001b[33mLogs will be saved locally.\u001b[0m\n",
            "INFO 2025-12-22 05:02:40 ot_train.py:183 Creating dataset\n",
            "INFO 2025-12-22 05:02:41 ot_train.py:202 Creating policy\n",
            "Loading  HuggingFaceTB/SmolVLM2-500M-Video-Instruct weights ...\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 2025-12-22 05:02:41 modeling.py:987 We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
            "Reducing the number of VLM layers to 16 ...\n",
            "INFO 2025-12-22 05:02:57 ot_train.py:247 Creating optimizer and scheduler\n",
            "INFO 2025-12-22 05:02:57 hedulers.py:105 Auto-scaling LR scheduler: num_training_steps (200) < num_decay_steps (30000). Scaling warmup: 1000 → 6, decay: 30000 → 200 (scale factor: 0.007)\n",
            "INFO 2025-12-22 05:02:57 ot_train.py:259 \u001b[1m\u001b[33mOutput dir:\u001b[0m /content/outputs/train/my_smolvla_l1\n",
            "INFO 2025-12-22 05:02:57 ot_train.py:264 cfg.steps=200 (200)\n",
            "INFO 2025-12-22 05:02:57 ot_train.py:265 dataset.num_frames=19631 (20K)\n",
            "INFO 2025-12-22 05:02:57 ot_train.py:266 dataset.num_episodes=50\n",
            "INFO 2025-12-22 05:02:57 ot_train.py:269 Effective batch size: 8 x 1 = 8\n",
            "INFO 2025-12-22 05:02:57 ot_train.py:270 num_learnable_params=99880992 (100M)\n",
            "INFO 2025-12-22 05:02:57 ot_train.py:271 num_total_params=450046176 (450M)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "INFO 2025-12-22 05:02:57 ot_train.py:327 Start offline training on a fixed dataset\n",
            "INFO 2025-12-22 05:10:45 ot_train.py:354 step:200 smpl:2K ep:4 epch:0.08 loss:0.060 grdn:0.978 lr:5.0e-05 updt_s:2.308 data_s:0.030\n",
            "INFO 2025-12-22 05:10:45 ot_train.py:364 Checkpoint policy after step 200\n",
            "INFO 2025-12-22 05:12:13 ot_train.py:435 End of training\n",
            "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            \n",
            "New Data Upload               : |          |  0.00B /  0.00B            \u001b[A\n",
            "\n",
            "  ...ning_l1/model.safetensors:   4% 33.6M/907M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :   4% 33.6M/907M [00:00<00:06, 128MB/s,   ???B/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :   7% 67.1M/907M [00:00<00:05, 149MB/s,  167MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  11% 101M/907M [00:00<00:05, 157MB/s,  168MB/s  ] \n",
            "\n",
            "Processing Files (0 / 1)      :  15% 134M/907M [00:00<00:04, 161MB/s,  168MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  18% 159M/907M [00:01<00:05, 148MB/s,  157MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  19% 176M/907M [00:01<00:05, 127MB/s,  143MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  22% 201M/907M [00:01<00:05, 127MB/s,  140MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  24% 218M/907M [00:01<00:06, 113MB/s,  132MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  27% 243M/907M [00:01<00:05, 117MB/s,  131MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  29% 260M/907M [00:02<00:06, 107MB/s,  126MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  31% 277M/907M [00:02<00:06, 99.9MB/s,  122MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  33% 302M/907M [00:02<00:05, 108MB/s,  122MB/s  ] \n",
            "\n",
            "Processing Files (0 / 1)      :  35% 319M/907M [00:02<00:05, 101MB/s,  119MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  37% 336M/907M [00:02<00:05, 95.5MB/s,  116MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  39% 352M/907M [00:03<00:06, 92.0MB/s,  114MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  42% 377M/907M [00:03<00:05, 102MB/s,  115MB/s  ] \n",
            "\n",
            "Processing Files (0 / 1)      :  43% 394M/907M [00:03<00:05, 96.5MB/s,  113MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  46% 419M/907M [00:03<00:04, 105MB/s,  113MB/s  ] \n",
            "\n",
            "Processing Files (0 / 1)      :  49% 445M/907M [00:03<00:04, 112MB/s,  114MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  53% 478M/907M [00:04<00:03, 128MB/s,  117MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  55% 503M/907M [00:04<00:03, 128MB/s,  117MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  59% 537M/907M [00:04<00:02, 140MB/s,  120MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  63% 570M/907M [00:04<00:02, 148MB/s,  122MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  67% 604M/907M [00:04<00:01, 154MB/s,  124MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  70% 638M/907M [00:05<00:01, 158MB/s,  126MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  74% 671M/907M [00:05<00:01, 161MB/s,  128MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  78% 705M/907M [00:05<00:01, 163MB/s,  129MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  81% 738M/907M [00:05<00:01, 164MB/s,  130MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  85% 772M/907M [00:05<00:00, 165MB/s,  132MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  88% 797M/907M [00:06<00:00, 154MB/s,  132MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  92% 830M/907M [00:06<00:00, 158MB/s,  133MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  96% 872M/907M [00:06<00:00, 173MB/s,  135MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      : 100% 906M/907M [00:06<00:00, 172MB/s,  136MB/s  ]\n",
            "\n",
            "Processing Files (1 / 1)      : 100% 907M/907M [00:06<00:00, 122MB/s,  132MB/s  ]\n",
            "\n",
            "  ...ning_l1/model.safetensors: 100% 907M/907M [00:06<00:00, 128MB/s]\u001b[A\u001b[A\n",
            "\n",
            "  ...ning_l1/model.safetensors: 100% 907M/907M [00:06<00:00, 125MB/s]\u001b[A\u001b[A\n",
            "\n",
            "  ...ning_l1/model.safetensors: 100% 907M/907M [00:07<00:00, 121MB/s]\u001b[A\u001b[A\n",
            "\n",
            "  ...ning_l1/model.safetensors: 100% 907M/907M [00:07<00:00, 118MB/s]\u001b[A\u001b[A\n",
            "\n",
            "  ...ning_l1/model.safetensors: 100% 907M/907M [00:07<00:00, 115MB/s]\u001b[A\u001b[A\n",
            "\n",
            "  ...ning_l1/model.safetensors: 100% 907M/907M [00:07<00:00, 114MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing Files (1 / 1)      : 100% 907M/907M [00:08<00:00, 112MB/s,  112MB/s  ]\n",
            "New Data Upload               : |          |  0.00B /  0.00B,  0.00B/s  \n",
            "  ...ning_l1/model.safetensors: 100% 907M/907M [00:07<00:00, 112MB/s]\n",
            "INFO 2025-12-22 05:13:07 etrained.py:237 Model pushed to https://huggingface.co/Sythen/my_smolvla_training_l1\n",
            "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            \n",
            "New Data Upload               : |          |  0.00B /  0.00B            \u001b[A\n",
            "\n",
            "  ...zer_processor.safetensors: 100% 3.75k/3.75k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing Files (1 / 1)      : 100% 3.75k/3.75k [00:00<00:00, 18.5kB/s,   ???B/s  ]\n",
            "\n",
            "  ...zer_processor.safetensors: 100% 3.75k/3.75k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing Files (1 / 1)      : 100% 3.75k/3.75k [00:00<00:00, 9.31kB/s,  0.00B/s  ]\n",
            "New Data Upload               : |          |  0.00B /  0.00B,  0.00B/s  \n",
            "  ...zer_processor.safetensors: 100% 3.75k/3.75k [00:00<?, ?B/s]\n",
            "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            \n",
            "New Data Upload               : |          |  0.00B /  0.00B            \u001b[A\n",
            "\n",
            "  ...zer_processor.safetensors: 100% 3.75k/3.75k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing Files (1 / 1)      : 100% 3.75k/3.75k [00:00<00:00, 18.5kB/s,   ???B/s  ]\n",
            "\n",
            "  ...zer_processor.safetensors: 100% 3.75k/3.75k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing Files (1 / 1)      : 100% 3.75k/3.75k [00:00<00:00, 9.30kB/s,  0.00B/s  ]\n",
            "New Data Upload               : |          |  0.00B /  0.00B,  0.00B/s  \n",
            "  ...zer_processor.safetensors: 100% 3.75k/3.75k [00:00<?, ?B/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lerobot-train \\\n",
        "  --config_path /content/outputs/train/my_smolvla/checkpoints/000200/pretrained_model/train_config.json \\\n",
        "  --policy.repo_id Sythen/my_smolvla_training_frozen \\\n",
        "  --policy.num_steps 200 \\\n",
        "  --policy.device cuda \\\n",
        "  --policy.push_to_hub true \\\n",
        "  --policy.freeze_vision_encoder true \\\n",
        "  --output_dir /content/outputs/train/my_smolvla_frozen\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GePvuMtgCjc",
        "outputId": "dea3bc95-4342-4fef-f888-082683ae51a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-22 05:21:20.867178: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766380880.889644   21049 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766380880.896498   21049 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766380880.913611   21049 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766380880.913654   21049 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766380880.913658   21049 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766380880.913662   21049 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-22 05:21:20.918617: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO 2025-12-22 05:21:31 ot_train.py:163 {'batch_size': 8,\n",
            " 'checkpoint_path': None,\n",
            " 'dataset': {'episodes': None,\n",
            "             'image_transforms': {'enable': False,\n",
            "                                  'max_num_transforms': 3,\n",
            "                                  'random_order': False,\n",
            "                                  'tfs': {'affine': {'kwargs': {'degrees': [-5.0,\n",
            "                                                                            5.0],\n",
            "                                                                'translate': [0.05,\n",
            "                                                                              0.05]},\n",
            "                                                     'type': 'RandomAffine',\n",
            "                                                     'weight': 1.0},\n",
            "                                          'brightness': {'kwargs': {'brightness': [0.8,\n",
            "                                                                                   1.2]},\n",
            "                                                         'type': 'ColorJitter',\n",
            "                                                         'weight': 1.0},\n",
            "                                          'contrast': {'kwargs': {'contrast': [0.8,\n",
            "                                                                               1.2]},\n",
            "                                                       'type': 'ColorJitter',\n",
            "                                                       'weight': 1.0},\n",
            "                                          'hue': {'kwargs': {'hue': [-0.05,\n",
            "                                                                     0.05]},\n",
            "                                                  'type': 'ColorJitter',\n",
            "                                                  'weight': 1.0},\n",
            "                                          'saturation': {'kwargs': {'saturation': [0.5,\n",
            "                                                                                   1.5]},\n",
            "                                                         'type': 'ColorJitter',\n",
            "                                                         'weight': 1.0},\n",
            "                                          'sharpness': {'kwargs': {'sharpness': [0.5,\n",
            "                                                                                 1.5]},\n",
            "                                                        'type': 'SharpnessJitter',\n",
            "                                                        'weight': 1.0}}},\n",
            "             'repo_id': 'lerobot/svla_so100_pickplace',\n",
            "             'revision': None,\n",
            "             'root': None,\n",
            "             'streaming': False,\n",
            "             'use_imagenet_stats': True,\n",
            "             'video_backend': 'torchcodec'},\n",
            " 'env': None,\n",
            " 'eval': {'batch_size': 50, 'n_episodes': 50, 'use_async_envs': False},\n",
            " 'eval_freq': 20000,\n",
            " 'job_name': 'my_smolvla_training_test',\n",
            " 'log_freq': 200,\n",
            " 'num_workers': 4,\n",
            " 'optimizer': {'betas': [0.9, 0.95],\n",
            "               'eps': 1e-08,\n",
            "               'grad_clip_norm': 10.0,\n",
            "               'lr': 0.0001,\n",
            "               'type': 'adamw',\n",
            "               'weight_decay': 1e-10},\n",
            " 'output_dir': '/content/outputs/train/my_smolvla_frozen',\n",
            " 'policy': {'adapt_to_pi_aloha': False,\n",
            "            'add_image_special_tokens': False,\n",
            "            'attention_mode': 'cross_attn',\n",
            "            'chunk_size': 50,\n",
            "            'device': 'cuda',\n",
            "            'empty_cameras': 0,\n",
            "            'expert_width_multiplier': 0.75,\n",
            "            'freeze_vision_encoder': True,\n",
            "            'input_features': {'observation.images.camera1': {'shape': [3,\n",
            "                                                                        256,\n",
            "                                                                        256],\n",
            "                                                              'type': <FeatureType.VISUAL: 'VISUAL'>},\n",
            "                               'observation.images.camera2': {'shape': [3,\n",
            "                                                                        256,\n",
            "                                                                        256],\n",
            "                                                              'type': <FeatureType.VISUAL: 'VISUAL'>},\n",
            "                               'observation.images.camera3': {'shape': [3,\n",
            "                                                                        256,\n",
            "                                                                        256],\n",
            "                                                              'type': <FeatureType.VISUAL: 'VISUAL'>},\n",
            "                               'observation.state': {'shape': [6],\n",
            "                                                     'type': <FeatureType.STATE: 'STATE'>}},\n",
            "            'license': None,\n",
            "            'load_vlm_weights': True,\n",
            "            'max_action_dim': 32,\n",
            "            'max_period': 4.0,\n",
            "            'max_state_dim': 32,\n",
            "            'min_period': 0.004,\n",
            "            'n_action_steps': 50,\n",
            "            'n_obs_steps': 1,\n",
            "            'normalization_mapping': {'ACTION': <NormalizationMode.MEAN_STD: 'MEAN_STD'>,\n",
            "                                      'STATE': <NormalizationMode.MEAN_STD: 'MEAN_STD'>,\n",
            "                                      'VISUAL': <NormalizationMode.IDENTITY: 'IDENTITY'>},\n",
            "            'num_expert_layers': 0,\n",
            "            'num_steps': 200,\n",
            "            'num_vlm_layers': 16,\n",
            "            'optimizer_betas': [0.9, 0.95],\n",
            "            'optimizer_eps': 1e-08,\n",
            "            'optimizer_grad_clip_norm': 10.0,\n",
            "            'optimizer_lr': 0.0001,\n",
            "            'optimizer_weight_decay': 1e-10,\n",
            "            'output_features': {'action': {'shape': [6],\n",
            "                                           'type': <FeatureType.ACTION: 'ACTION'>}},\n",
            "            'pad_language_to': 'max_length',\n",
            "            'prefix_length': 0,\n",
            "            'pretrained_path': 'lerobot/smolvla_base',\n",
            "            'private': None,\n",
            "            'push_to_hub': True,\n",
            "            'repo_id': 'Sythen/my_smolvla_training_frozen',\n",
            "            'resize_imgs_with_padding': [512, 512],\n",
            "            'rtc_config': None,\n",
            "            'scheduler_decay_lr': 2.5e-06,\n",
            "            'scheduler_decay_steps': 30000,\n",
            "            'scheduler_warmup_steps': 1000,\n",
            "            'self_attn_every_n_layers': 2,\n",
            "            'tags': None,\n",
            "            'tokenizer_max_length': 48,\n",
            "            'train_expert_only': True,\n",
            "            'train_state_proj': True,\n",
            "            'type': 'smolvla',\n",
            "            'use_amp': False,\n",
            "            'use_cache': True,\n",
            "            'use_delta_joint_actions_aloha': False,\n",
            "            'vlm_model_name': 'HuggingFaceTB/SmolVLM2-500M-Video-Instruct'},\n",
            " 'rename_map': {'observation.images.top': 'observation.images.camera1',\n",
            "                'observation.images.wrist': 'observation.images.camera2'},\n",
            " 'resume': False,\n",
            " 'save_checkpoint': True,\n",
            " 'save_freq': 20000,\n",
            " 'scheduler': {'decay_lr': 2.5e-06,\n",
            "               'num_decay_steps': 30000,\n",
            "               'num_warmup_steps': 1000,\n",
            "               'peak_lr': 0.0001,\n",
            "               'type': 'cosine_decay_with_warmup'},\n",
            " 'seed': 1000,\n",
            " 'steps': 200,\n",
            " 'use_policy_training_preset': True,\n",
            " 'wandb': {'disable_artifact': False,\n",
            "           'enable': False,\n",
            "           'entity': None,\n",
            "           'mode': None,\n",
            "           'notes': None,\n",
            "           'project': 'lerobot',\n",
            "           'run_id': None}}\n",
            "INFO 2025-12-22 05:21:31 ot_train.py:171 \u001b[1m\u001b[33mLogs will be saved locally.\u001b[0m\n",
            "INFO 2025-12-22 05:21:31 ot_train.py:183 Creating dataset\n",
            "INFO 2025-12-22 05:21:31 ot_train.py:202 Creating policy\n",
            "Loading  HuggingFaceTB/SmolVLM2-500M-Video-Instruct weights ...\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "INFO 2025-12-22 05:21:31 modeling.py:987 We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
            "Reducing the number of VLM layers to 16 ...\n",
            "INFO 2025-12-22 05:21:50 ot_train.py:247 Creating optimizer and scheduler\n",
            "INFO 2025-12-22 05:21:50 hedulers.py:105 Auto-scaling LR scheduler: num_training_steps (200) < num_decay_steps (30000). Scaling warmup: 1000 → 6, decay: 30000 → 200 (scale factor: 0.007)\n",
            "INFO 2025-12-22 05:21:50 ot_train.py:259 \u001b[1m\u001b[33mOutput dir:\u001b[0m /content/outputs/train/my_smolvla_frozen\n",
            "INFO 2025-12-22 05:21:50 ot_train.py:264 cfg.steps=200 (200)\n",
            "INFO 2025-12-22 05:21:50 ot_train.py:265 dataset.num_frames=19631 (20K)\n",
            "INFO 2025-12-22 05:21:50 ot_train.py:266 dataset.num_episodes=50\n",
            "INFO 2025-12-22 05:21:50 ot_train.py:269 Effective batch size: 8 x 1 = 8\n",
            "INFO 2025-12-22 05:21:50 ot_train.py:270 num_learnable_params=99880992 (100M)\n",
            "INFO 2025-12-22 05:21:50 ot_train.py:271 num_total_params=450046176 (450M)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "INFO 2025-12-22 05:21:50 ot_train.py:327 Start offline training on a fixed dataset\n",
            "INFO 2025-12-22 05:29:36 ot_train.py:354 step:200 smpl:2K ep:4 epch:0.08 loss:0.060 grdn:0.978 lr:5.0e-05 updt_s:2.304 data_s:0.026\n",
            "INFO 2025-12-22 05:29:36 ot_train.py:364 Checkpoint policy after step 200\n",
            "INFO 2025-12-22 05:29:54 ot_train.py:435 End of training\n",
            "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            \n",
            "New Data Upload               : |          |  0.00B /  0.00B            \u001b[A\n",
            "\n",
            "  ..._frozen/model.safetensors:   4% 33.6M/907M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing Files (0 / 1)      :   4% 33.6M/907M [00:00<00:05, 155MB/s,   ???B/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :   7% 67.1M/907M [00:00<00:05, 162MB/s,  168MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  11% 101M/907M [00:00<00:04, 165MB/s,  168MB/s  ] \n",
            "\n",
            "Processing Files (0 / 1)      :  15% 134M/907M [00:00<00:04, 166MB/s,  168MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  18% 168M/907M [00:01<00:04, 167MB/s,  168MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  22% 201M/907M [00:01<00:04, 167MB/s,  168MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  25% 226M/907M [00:01<00:04, 153MB/s,  161MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  27% 243M/907M [00:01<00:05, 132MB/s,  150MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  30% 268M/907M [00:01<00:04, 129MB/s,  147MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  31% 285M/907M [00:02<00:05, 116MB/s,  140MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  34% 310M/907M [00:02<00:05, 119MB/s,  138MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  37% 336M/907M [00:02<00:04, 121MB/s,  137MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  39% 352M/907M [00:02<00:05, 110MB/s,  133MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  42% 377M/907M [00:02<00:04, 115MB/s,  132MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  43% 394M/907M [00:03<00:04, 105MB/s,  129MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  46% 419M/907M [00:03<00:04, 111MB/s,  129MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  48% 436M/907M [00:03<00:04, 103MB/s,  126MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  51% 461M/907M [00:03<00:04, 110MB/s,  126MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  53% 478M/907M [00:03<00:04, 102MB/s,  123MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  55% 503M/907M [00:04<00:03, 109MB/s,  124MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  59% 537M/907M [00:04<00:02, 127MB/s,  126MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  63% 570M/907M [00:04<00:02, 139MB/s,  128MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  67% 604M/907M [00:04<00:02, 148MB/s,  130MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  70% 638M/907M [00:04<00:01, 154MB/s,  131MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  74% 671M/907M [00:05<00:01, 158MB/s,  133MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  78% 705M/907M [00:05<00:01, 161MB/s,  134MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  81% 738M/907M [00:05<00:01, 163MB/s,  136MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  85% 772M/907M [00:05<00:00, 164MB/s,  137MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  90% 814M/907M [00:05<00:00, 178MB/s,  139MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  93% 847M/907M [00:06<00:00, 174MB/s,  140MB/s  ]\n",
            "\n",
            "Processing Files (0 / 1)      :  97% 881M/907M [00:06<00:00, 173MB/s,  141MB/s  ]\n",
            "\n",
            "Processing Files (1 / 1)      : 100% 907M/907M [00:06<00:00, 160MB/s,  141MB/s  ]\n",
            "\n",
            "  ..._frozen/model.safetensors: 100% 907M/907M [00:06<00:00, 136MB/s]\u001b[A\u001b[A\n",
            "\n",
            "  ..._frozen/model.safetensors: 100% 907M/907M [00:06<00:00, 132MB/s]\u001b[A\u001b[A\n",
            "\n",
            "  ..._frozen/model.safetensors: 100% 907M/907M [00:06<00:00, 128MB/s]\u001b[A\u001b[A\n",
            "\n",
            "  ..._frozen/model.safetensors: 100% 907M/907M [00:07<00:00, 125MB/s]\u001b[A\u001b[A\n",
            "\n",
            "  ..._frozen/model.safetensors: 100% 907M/907M [00:07<00:00, 121MB/s]\u001b[A\u001b[A\n",
            "\n",
            "  ..._frozen/model.safetensors: 100% 907M/907M [00:07<00:00, 118MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing Files (1 / 1)      : 100% 907M/907M [00:07<00:00, 119MB/s,  118MB/s  ]\n",
            "New Data Upload               : |          |  0.00B /  0.00B,  0.00B/s  \n",
            "  ..._frozen/model.safetensors: 100% 907M/907M [00:07<00:00, 118MB/s]\n",
            "INFO 2025-12-22 05:31:17 etrained.py:237 Model pushed to https://huggingface.co/Sythen/my_smolvla_training_frozen\n",
            "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            \n",
            "New Data Upload               : |          |  0.00B /  0.00B            \u001b[A\n",
            "\n",
            "  ...zer_processor.safetensors: 100% 3.75k/3.75k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing Files (1 / 1)      : 100% 3.75k/3.75k [00:00<00:00, 18.5kB/s,   ???B/s  ]\n",
            "\n",
            "  ...zer_processor.safetensors: 100% 3.75k/3.75k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing Files (1 / 1)      : 100% 3.75k/3.75k [00:00<00:00, 9.31kB/s,  0.00B/s  ]\n",
            "New Data Upload               : |          |  0.00B /  0.00B,  0.00B/s  \n",
            "  ...zer_processor.safetensors: 100% 3.75k/3.75k [00:00<?, ?B/s]\n",
            "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            \n",
            "New Data Upload               : |          |  0.00B /  0.00B            \u001b[A\n",
            "\n",
            "  ...zer_processor.safetensors: 100% 3.75k/3.75k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing Files (1 / 1)      : 100% 3.75k/3.75k [00:00<00:00, 18.5kB/s,   ???B/s  ]\n",
            "\n",
            "  ...zer_processor.safetensors: 100% 3.75k/3.75k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "Processing Files (1 / 1)      : 100% 3.75k/3.75k [00:00<00:00, 9.29kB/s,  0.00B/s  ]\n",
            "New Data Upload               : |          |  0.00B /  0.00B,  0.00B/s  \n",
            "  ...zer_processor.safetensors: 100% 3.75k/3.75k [00:00<?, ?B/s]\n"
          ]
        }
      ]
    }
  ]
}